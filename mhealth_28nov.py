# -*- coding: utf-8 -*-
"""mHealth_28Nov.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJlxbZMG6tg1bkl7khG3U9RWl1ngvyv7

**Drive Mounting and setup**
"""

# MOUNT DRIVE & INSTALL PACKAGES
from google.colab import drive
drive.mount('/content/drive')

!pip install xgboost --quiet

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier

import xgboost as xgb

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

print("TensorFlow version:", tf.__version__)

"""**Set Paths**"""

# 1. PATHS & CONFIG

# Folder in Drive where your *.log files live
# Example: "/content/drive/MyDrive/MHEALTHDATASET"
RAW_DATA_DIR = "/content/drive/MyDrive/MHEALTHDATASET"

# Combined CSV path
COMBINED_CSV = "/content/drive/MyDrive/mhealth_combined.csv"

# Where to save trained models
MODEL_DIR = "/content/drive/MyDrive/mhealth_models"
os.makedirs(MODEL_DIR, exist_ok=True)

# Sampling info & window config
FS = 50        # Hz (MHEALTH is 50Hz)
WIN_SEC = 2.0  # seconds per window
WIN_SIZE = int(FS * WIN_SEC)  # samples
OVERLAP = 0.5
STEP = int(WIN_SIZE * (1 - OVERLAP))

print("WIN_SIZE:", WIN_SIZE, "STEP:", STEP)

ACTIVITY_MAP = {
    1: "Standing still",
    2: "Sitting and relaxing",
    3: "Lying down",
    4: "Walking",
    5: "Climbing stairs",
    6: "Waist bends forward",
    7: "Frontal elevation of arms",
    8: "Knees bending (crouching)",
    9: "Cycling",
    10: "Jogging",
    11: "Running",
    12: "Jump front & back",
}

"""**Combine all .log files into single csv**"""

# 2. COMBINE ALL .LOG FILES INTO ONE CSV

def combine_logs_to_csv(raw_dir, out_csv):
    """
    Combines all .log files into one CSV with 'subject' column.
    If CSV exists, loads it.
    """
    if os.path.exists(out_csv):
        print("Combined CSV exists, loading:", out_csv)
        return pd.read_csv(out_csv)

    import glob
    log_files = glob.glob(os.path.join(raw_dir, "*.log"))
    if not log_files:
        raise FileNotFoundError(f"No .log files found in {raw_dir}")

    # MHEALTH standard 23 columns + activity
    columns = [
        'acc_chest_x','acc_chest_y','acc_chest_z',
        'ecg_1','ecg_2',
        'acc_l_x','acc_l_y','acc_l_z',
        'gyro_l_x','gyro_l_y','gyro_l_z',
        'mag_l_x','mag_l_y','mag_l_z',
        'acc_r_x','acc_r_y','acc_r_z',
        'gyro_r_x','gyro_r_y','gyro_r_z',
        'mag_r_x','mag_r_y','mag_r_z',
        'activity'
    ]

    dfs = []
    for f in log_files:
        df = pd.read_csv(f, delim_whitespace=True, header=None)
        if df.shape[1] < len(columns):
            raise ValueError(f"File {f} has {df.shape[1]} cols, expected >= {len(columns)}")

        df = df.iloc[:, :len(columns)]
        df.columns = columns

        # subject from file name: mHealth_subject1.log → subject1
        subj = os.path.basename(f).split("_")[1].replace(".log", "")
        df["subject"] = subj
        dfs.append(df)

    full = pd.concat(dfs, ignore_index=True)
    full.to_csv(out_csv, index=False)
    print("Saved combined CSV:", out_csv, "shape:", full.shape)
    return full

df = combine_logs_to_csv(RAW_DATA_DIR, COMBINED_CSV)
df.head()

"""**Data Cleaning Pipeline**"""

# 3. DATA CLEANING

def basic_cleaning(df):
    df = df.copy()

    # Remove duplicates
    df = df.drop_duplicates()

    # Sensor columns = all except activity + subject
    sensor_cols = [c for c in df.columns if c not in ("activity", "subject")]

    # Replace 0 in sensors (optional) with NaN
    df[sensor_cols] = df[sensor_cols].replace(0, np.nan)

    # Drop rows that are almost entirely NaN
    df = df.dropna(thresh=len(sensor_cols) - 3)

    # Fill remaining NaNs with median
    df[sensor_cols] = df[sensor_cols].fillna(df[sensor_cols].median())

    return df, sensor_cols

df_clean, sensor_cols = basic_cleaning(df)
print("After cleaning:", df_clean.shape)
df_clean.head()

"""**Load the csv and show a few records**"""

df = pd.read_csv("/content/drive/MyDrive/mhealth_combined.csv")

# Show first 5 rows
df.head()

"""** SLIDING WINDOW PIPELINE**"""

# 4. SLIDING WINDOW – PER SUBJECT

def sliding_window_array(X, y, win_size, step):
    """
    X: (N, C) sensor data
    y: (N,) activity labels
    Returns:
      Xw: (n_windows, win_size, C)
      yw: (n_windows,)
    """
    Xw, yw = [], []
    for start in range(0, len(X) - win_size + 1, step):
        end = start + win_size
        win = X[start:end]
        lab = y[start:end]
        # majority label in window
        labels, counts = np.unique(lab, return_counts=True)
        majority = labels[np.argmax(counts)]
        Xw.append(win)
        yw.append(majority)
    return np.array(Xw), np.array(yw)


X_list, y_list = [], []
for subj in df_clean["subject"].unique():
    df_sub = df_clean[df_clean["subject"] == subj].reset_index(drop=True)
    X_sub = df_sub[sensor_cols].values
    y_sub = df_sub["activity"].values.astype(int)

    Xw_sub, yw_sub = sliding_window_array(X_sub, y_sub, WIN_SIZE, STEP)
    X_list.append(Xw_sub)
    y_list.append(yw_sub)
    print(f"Subject {subj}: windows = {Xw_sub.shape[0]}")

X_all = np.vstack(X_list)      # (N_win, T, C)
y_all = np.hstack(y_list)      # (N_win,)

print("All windows:", X_all.shape, "labels:", y_all.shape)

"""**Feature extraction for ML (RF/XGBoost)**"""

# 5. FEATURE EXTRACTION FOR CLASSICAL ML

import numpy as np
# Removed: from numba import njit

# Removed: @njit
def fast_skew(x, mean, std):
    if std == 0:
        return 0.0
    n = x.size
    return np.sum(((x - mean) / std) ** 3) / n

# Removed: @njit
def fast_kurtosis(x, mean, std):
    if std == 0:
        return 0.0
    n = x.size
    return np.sum(((x - mean) / std) ** 4) / n - 3.0

# Removed: @njit
def extract_features_window_fast(win, fs):
    T, C = win.shape
    features_per_ch = 11
    feats = np.empty(C * features_per_ch, dtype=np.float64)

    idx = 0

    for c in range(C):
        x = win[:, c]

        # ---- time-domain ----
        mean = x.mean()
        std = x.std()
        mn = x.min()
        mx = x.max()
        median = np.median(x)
        rms = np.sqrt((x * x).mean())
        p2p = mx - mn

        skew_val = fast_skew(x, mean, std)
        kurt_val = fast_kurtosis(x, mean, std)

        # ---- freq-domain ----
        yf = np.abs(np.fft.rfft(x))

        if yf.size > 0:
            peak_bin = np.argmax(yf)
            peak_freq = peak_bin * (fs / T)
            power = np.sum(yf * yf) / yf.size
        else:
            peak_freq = 0.0
            power = 0.0

        # store them
        feats[idx] = mean
        feats[idx+1] = std
        feats[idx+2] = mn
        feats[idx+3] = mx
        feats[idx+4] = median
        feats[idx+5] = rms
        feats[idx+6] = skew_val
        feats[idx+7] = kurt_val
        feats[idx+8] = p2p
        feats[idx+9] = peak_freq
        feats[idx+10] = power

        idx += features_per_ch

    return feats


# Removed: @njit
def build_feature_matrix_fast(X_windows, fs):
    N = len(X_windows)
    T, C = X_windows[0].shape
    features_per_ch = 11
    Xf = np.empty((N, C * features_per_ch), dtype=np.float64)

    for i in range(N):
        Xf[i] = extract_features_window_fast(X_windows[i], fs)

    return Xf
X_feats = build_feature_matrix_fast(X_all, FS)
print("Feature matrix:", X_feats.shape)   # (N_win, C * features_per_channel)

"""**Train/test split and scaling**"""

# 6. TRAIN/TEST SPLIT + SCALING

Xf_train, Xf_test, yf_train, yf_test = train_test_split(
    X_feats, y_all, test_size=0.2, stratify=y_all, random_state=42
)

scaler = StandardScaler()
Xf_train_s = scaler.fit_transform(Xf_train)
Xf_test_s = scaler.transform(Xf_test)

print("Train:", Xf_train_s.shape, "Test:", Xf_test_s.shape)

"""**RandomForest Baseline**"""

# 7. RANDOMFOREST BASELINE

rf = RandomForestClassifier(
    n_estimators=300,
    n_jobs=-1,
    random_state=42
)
rf.fit(Xf_train_s, yf_train)
yf_pred_rf = rf.predict(Xf_test_s)

print("=== RandomForest ===")
print("Accuracy:", accuracy_score(yf_test, yf_pred_rf))
print(classification_report(yf_test, yf_pred_rf, digits=3))

cm_rf = confusion_matrix(yf_test, yf_pred_rf)
plt.figure(figsize=(6,5))
plt.imshow(cm_rf, interpolation='nearest')
plt.title("RandomForest confusion matrix")
plt.colorbar()
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Save RF + scaler
import joblib
joblib.dump(rf, os.path.join(MODEL_DIR, "rf_model.joblib"))
joblib.dump(scaler, os.path.join(MODEL_DIR, "rf_scaler.joblib"))
print("Saved RF and scaler to", MODEL_DIR)

"""**LSTM-**-> Long Short Term Memory on raw windows (Deep Learning)

*   It is a special type of Recurrent Neural Network (RNN) designed to understand time-dependent patterns.

**Think of LSTM like a smart memory box:**

It remembers important information for a long time

It forgets useless information

It focuses on useful trends
We compare RF on features Vs LSTM on raw sequences

"""

# 8. LSTM ON RAW WINDOWS (DEEP LEARNING)

Xr_train, Xr_test, yr_train, yr_test = train_test_split(
    X_all, y_all, test_size=0.2, stratify=y_all, random_state=42
)

n_timesteps = Xr_train.shape[1]
n_channels = Xr_train.shape[2]
n_classes = len(np.unique(y_all))

print("Input for LSTM:", n_timesteps, "timesteps,", n_channels, "channels,", n_classes, "classes")

# map labels to 0..(C-1)
unique_labels = np.sort(np.unique(y_all))
label2idx = {lab: i for i, lab in enumerate(unique_labels)}
idx2label = {i: lab for lab, i in label2idx.items()}

yr_train_idx = np.array([label2idx[l] for l in yr_train])
yr_test_idx  = np.array([label2idx[l] for l in yr_test])

yr_train_cat = tf.keras.utils.to_categorical(yr_train_idx, num_classes=n_classes)
yr_test_cat  = tf.keras.utils.to_categorical(yr_test_idx,  num_classes=n_classes)

def build_lstm(input_shape, n_classes):
    model = Sequential()
    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.3))
    model.add(LSTM(64))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(n_classes, activation="softmax"))
    model.compile(
        loss="categorical_crossentropy",
        optimizer="adam",
        metrics=["accuracy"]
    )
    return model

input_shape = (n_timesteps, n_channels)
lstm_model = build_lstm(input_shape, n_classes)
lstm_model.summary()

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True
    )
]

history = lstm_model.fit(
    Xr_train, yr_train_cat,
    epochs=30,
    batch_size=64,
    validation_split=0.2,
    callbacks=callbacks,
    verbose=1
)

# Plot accuracy curve
plt.figure()
plt.plot(history.history["accuracy"], label="train_acc")
plt.plot(history.history["val_accuracy"], label="val_acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("LSTM training")
plt.show()

# Evaluate
loss, acc = lstm_model.evaluate(Xr_test, yr_test_cat, verbose=0)
print("LSTM test accuracy:", acc)

# Predictions
y_proba_lstm = lstm_model.predict(Xr_test)
y_pred_lstm_idx = np.argmax(y_proba_lstm, axis=1)
y_pred_lstm = np.array([idx2label[i] for i in y_pred_lstm_idx])

print("=== LSTM ===")
print("Accuracy:", accuracy_score(yr_test, y_pred_lstm))
print(classification_report(yr_test, y_pred_lstm, digits=3))

cm_lstm = confusion_matrix(yr_test, y_pred_lstm)
plt.figure(figsize=(6,5))
plt.imshow(cm_lstm, interpolation='nearest')
plt.title("LSTM confusion matrix")
plt.colorbar()
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Save model
lstm_model.save(os.path.join(MODEL_DIR, "lstm_model.h5"))
print("Saved LSTM to", MODEL_DIR)

"""**RandomForest Vs LSTM**"""

# 9. COMPARISON SUMMARY

rf_acc   = accuracy_score(yf_test, yf_pred_rf)
lstm_acc = accuracy_score(yr_test, y_pred_lstm)

print("=== Model accuracy comparison ===")
print(f"RandomForest (stats features): {rf_acc:.4f}")
print(f"LSTM (raw windows)          : {lstm_acc:.4f}")

"""**Feature importance per sensor node (chest / ankle / wrist)**
* We use RF’s feature_importances_ and sum over all features belonging to each body location.

"""

# 10. FEATURE IMPORTANCE PER SENSOR NODE (CHEST / ANKLE / WRIST)

# true sensor columns used to build X_all
true_sensor_cols = [c for c in df_clean.columns if c not in ("activity", "subject")]
C = len(true_sensor_cols)
print("Sensor channels:", C)
print(true_sensor_cols)

# In extract_features_from_window, we used:
# 9 time features + 2 freq features = 11 features per channel
FEAT_PER_CH = 11

def channel_to_feature_indices(ch_idx):
    base = ch_idx * FEAT_PER_CH
    return list(range(base, base + FEAT_PER_CH))

# Group channels by body location using name patterns
chest_ch_idx = [i for i, c in enumerate(true_sensor_cols) if "chest" in c]
ankle_ch_idx = [i for i, c in enumerate(true_sensor_cols) if "_l_" in c or "acc_l" in c or "gyro_l" in c or "mag_l" in c]
wrist_ch_idx = [i for i, c in enumerate(true_sensor_cols) if "_r_" in c or "acc_r" in c or "gyro_r" in c or "mag_r" in c]

groups = {
    "chest": chest_ch_idx,
    "ankle": ankle_ch_idx,
    "wrist": wrist_ch_idx,
}
print("Channel groups:", groups)

rf_importance = rf.feature_importances_
node_importance = {}

for node_name, ch_list in groups.items():
    feat_idx_all = []
    for ch in ch_list:
        feat_idx_all.extend(channel_to_feature_indices(ch))
    if feat_idx_all:
        node_importance[node_name] = rf_importance[feat_idx_all].sum()
    else:
        node_importance[node_name] = 0.0

print("Node-level importance (sum of feature_importances_):")
for k, v in node_importance.items():
    print(f"{k}: {v:.4f}")

plt.figure()
plt.bar(node_importance.keys(), node_importance.values())
plt.ylabel("Importance (sum)")
plt.title("RandomForest feature importance per sensor node")
plt.show()

"""**Test the code**"""